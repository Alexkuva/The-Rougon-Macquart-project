{"cells":[{"cell_type":"markdown","source":["#Computing total storage size of a folder in Azure Data Lake withÂ Pyspark"],"metadata":{}},{"cell_type":"markdown","source":["<div style=\"float:right\"><img src=\"https://github.com/Alexkuva/The-Rougon-Macquart-project/blob/dev/Architecture/Azure/Data%20Lake/computing-total-storage/_img/compute-total-storage-architecture.png?raw=true\" width=\"500\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["## Datalake API's configuration"],"metadata":{}},{"cell_type":"code","source":["#Load libraries\n#--you must load azure.datalake.store into your cluster first\nfrom azure.datalake.store import core, lib"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Declare variables\ndirectory_id = \"\"\napplication_key = \"\"\napplication_id = \"\"\nadls_name = \"\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Connect to Azure\nadls_credentials = lib.auth(tenant_id=directory_id, client_secret=application_key, client_id=application_id)\n#Create the connection\nadls_client = core.AzureDLFileSystem(adls_credentials, store_name=adls_name)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["print(\"Configuration API ADLS :\")\nprint(adls_name)\nprint(adls_client.listdir())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["##Define functions"],"metadata":{}},{"cell_type":"code","source":["#Load libraries\nsql(\"set spark.sql.execution.arrow.enabled true\")\nfrom pyspark.sql.functions import concat, col, lit, pandas_udf"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#Total size for a path\ndef recursiveDirSize(path):\n  total = 0\n  dir_files = adls_client.listdir(path=path,detail=True)\n  for file in dir_files:\n    if file['type']=='DIRECTORY':\n      total += recursiveDirSize(file['name'])\n    else:\n      total += file['length']\n  return total\n#UDF\nudfRecursiveDirSize = udf(recursiveDirSize)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Number of files for a path\ndef recursiveNbFile(path):\n  total = 0\n  dir_files = adls_client.listdir(path=path,detail=True)\n  for file in dir_files:\n    if file['type']=='DIRECTORY':\n      total += recursiveNbFile(file['name'])\n    else:\n      total += 1\n  return total\n#UDF\nudfrecursiveNbFile = udf(recursiveNbFile)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Number of folders for a path\ndef recursiveNbFolder(path):\n  total = 0\n  dir_files = adls_client.listdir(path=path,detail=True)\n  for file in dir_files:\n    if file['type']=='DIRECTORY':\n      total += 1\n      total += recursiveNbFolder(file['name'])\n  return total\n#UDF\nudfrecursiveNbFolder = udf(recursiveNbFolder)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Load your dataframe"],"metadata":{}},{"cell_type":"code","source":["DataLakePath = \"\"\nenvironment = \"\""],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#Define schema\ndfSchema = StructType([\n  StructField(\"datasource\", StringType(), True),\n  StructField(\"path\", StringType(), True),\n  StructField(\"env\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Init dataframe\ndf = sqlContext.createDataFrame(sc.emptyRDD(), dfSchema)\nif len(adls_client.listdir(path=DataLakePath, detail=True, invalidate_cache=True)) != 0:\n  df = spark.createDataFrame(adls_client.listdir(path=DataLakePath, detail=True, invalidate_cache=True))\n  df  = (df\n    .withColumn('env',lit(environment))\n    .select(col(\"pathSuffix\").alias(\"datasource\"),col(\"name\").alias(\"path\"),col(\"env\"))\n  )\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Add storage information to the dataframe\n(\ndf\n   .withColumn('size',udfRecursiveDirSize(col('path')))\n   .withColumn('nbFiles',udfrecursiveNbFile(col('path')))\n   .withColumn('nbFolder',udfrecursiveNbFolder(col('path')))\n)"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"compute-total-storage-azure-datalake","notebookId":2562239506865445},"nbformat":4,"nbformat_minor":0}
